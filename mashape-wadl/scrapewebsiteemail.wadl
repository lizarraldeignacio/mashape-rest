<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<application xmlns:f2790="urn:function:f2790">
    <grammars>
        <include href="Function2790.xsd"/>
    </grammars>
    <doc>scrape emails from a website programmatically We are an api that scrapes emails from websites What it does Scrapes a website for you We go through the website and scrapes the main page and any sub pages for e mails and returns those to you in json Parses common e mail patterns We parse e mails such as tom at site com or tom at site dot com Let us handle it Easy API Just provide a website address as input and you re all set Excellent Support Let us know if you have issues We ll be on call We also have monitor on our API About the endpoint Please note that subsequent calls to the same url will be fetched from the cache flushed weekly Please do note we cannot parse sites that require a login for now so for some Facebook pages it is not possible at the moment to fetch the e mail Finally please note that a scrape will be terminated after minutes if data cannot be fetched at that time More Detailed Information Returns a list of emails scraped by priority ie e mails appear on top level pages are first Please note that subsequent calls to the same url will be fetched from the cache day flush Will also parse patterns such as hello at site com hello at site dot com hello at site com hello at site dot com hello site com hello site com Please do note we cannot parse sites that require a login for now so for some Facebook pages it is not possible at the moment to fetch the e mail Finally please note that the api will fetch links for up to minutes After that time it will start analysing the pages which have been scraped Therefore please ensure that your client has a timeout of at least seconds mins to fetch and the rest to parse Please note that due to the fact that the api goes out to fetch the pages the server allows only concurrent request ip Requests which are made while the request is still processing will result in a code</doc>
    <resources base="https://scrape-website-email.p.mashape.com/">
        <resource uri="scrape_emails.json/">
            <method displayName="Scrape Emails" id="2790" name="GET">
                <doc>Returns a list of emails scraped by priority (ie. e-mails appear on top level pages are first). Please note that subsequent calls to the same url will be fetched from the cache (14 day flush).   Will also parse patterns such as hello[at]site.com, hello[at]site[dot]com, hello(at)site.com, hello(at)site(dot)com, hello @ site.com, hello @ site . com.   Please do note we cannot parse sites that require a login (for now), so for some Facebook pages it is not possible at the moment to fetch the e-mail.  Finally, please note that the api will fetch links for up to 2 minutes. After that time it will start analysing the pages which have been scraped. Therefore please ensure that your client has a timeout of at least greater than 150 seconds (2 mins to fetch and the rest to parse).   Please note that due to the fact that the api goes out to fetch the pages, the server allows only 1 concurrent request/ip. Requests which are made while the 1 request is still processing will result in a 429 code.</doc>
                <request>
                    <param name="mustInclude" required="true" style="query" type="xsd:string">
                        <doc>a value that must be included in the page in order for the scrape to happen This is good in situations where you want certain context before scraping ie contact page</doc>
                    </param>
                    <param name="website" required="true" style="query" type="xsd:string">
                        <doc>the website to scrape It will return an object that contains the data field which has all the emails it found on the site and the children</doc>
                    </param>
                </request>
                <response>
                    <representation element="f2790:root" mediaType="application/json"/>
                </response>
            </method>
        </resource>
    </resources>
</application>
